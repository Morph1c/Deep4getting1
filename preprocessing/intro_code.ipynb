{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#for gan...\n",
    "!pip install visualkeras\n",
    "import visualkeras\n",
    "\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "def add_noise(cop):\n",
    "    imagee = copy.copy(cop)\n",
    "    var = random.randint(0, 1)\n",
    "    if var == 1:     \n",
    "        '''Add random noise to an image'''\n",
    "        imagee = imagee/255\n",
    "        VARIABILITY = 0.02\n",
    "        deviation = VARIABILITY\n",
    "        noise = np.random.normal(0, deviation, imagee.shape)\n",
    "        imagee += noise\n",
    "        # np.clip(img, 0., 255.)\n",
    "        return imagee*255\n",
    "    else:\n",
    "        lista = [10, 20, 25, 30, 35]\n",
    "\n",
    "        l = random.choice(lista)\n",
    "        box = np.zeros(l**2).reshape(l, l)\n",
    "        xx, yy = np.random.randint(0,95-l, size=2)\n",
    "        for i in range(3):\n",
    "            imagee[xx:xx+l, yy:yy+l, i] = box\n",
    "\n",
    "        return imagee\n",
    "    \n",
    "def orthogonal_rot(cop):\n",
    "    imagee = copy.copy(cop)\n",
    "    return np.rot90(imagee, np.random.choice([-1, 0, 1]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([3199, 2001], dtype=int64))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load images from the 'items/' folder\n",
    "temp = np.load('public_data.npz', allow_pickle = True)\n",
    "img = temp[\"data\"]\n",
    "label = temp[\"labels\"]\n",
    "  # Normalize image pixel values to a float range [0, 1]\n",
    "img = (img).astype(np.float32)\n",
    "\n",
    "# set 0,1 label\n",
    "for i in range(len(label)):\n",
    "    if(label[i] == 'healthy'):\n",
    "        label[i] = 0\n",
    "    else:\n",
    "        label[i] = 1\n",
    "\n",
    "# cleaning images from trol and shrek\n",
    "ref_img = img[58]\n",
    "ref_img2 = img[2150]\n",
    "c = 0\n",
    "c2 = 0\n",
    "rm_indexes = []\n",
    "rm2_indexes = []\n",
    "for i in range(0, len(img)):\n",
    "    deviation = np.mean(np.abs(ref_img - img[i]))\n",
    "    deviation2 = np.mean(np.abs(ref_img2 - img[i]))\n",
    "    if(deviation == 0.0):\n",
    "        #print(i)\n",
    "        c += 1\n",
    "        rm_indexes.append(i)\n",
    "    elif(deviation2 == 0.0):\n",
    "        c2 += 1\n",
    "        rm2_indexes.append(i)\n",
    "\n",
    "clean_img = np.delete(img, rm_indexes + rm2_indexes, axis=0)\n",
    "clean_label = np.delete(label, rm_indexes + rm2_indexes, axis=0)\n",
    "\n",
    "\n",
    "#DATA AUGMENTATION TO BALANCE CLASSES!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Create an instance of ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    # rotation_range=20,\n",
    "    # shear_range=[0.3, 0.7],\n",
    "#     zoom_range=[0.3, 0.9],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    # brightness_range=[0, 0.7],\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function = orthogonal_rot\n",
    ")\n",
    "\n",
    "# Reshape the numpy array to fit the requirements of the flow method\n",
    "x = clean_img[clean_label == 1]  # Replace 'your_numpy_array' with your actual numpy array\n",
    "\n",
    "# Create a generator to augment the images\n",
    "augmented_images = []\n",
    "for x_batch in datagen.flow(x, batch_size=1, seed=42, shuffle=False):\n",
    "    augmented_images.append(x_batch[0])\n",
    "    if len(augmented_images) >= 1198:\n",
    "        break\n",
    "\n",
    "# Convert the list of augmented images to a numpy array\n",
    "augmented_images_array = np.array(augmented_images)\n",
    "\n",
    "clean_img = np.append(clean_img, augmented_images_array, axis = 0)\n",
    "clean_label = np.append(clean_label, np.ones(1198))\n",
    "print(clean_img.shape)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "# Concatenate 'animals' and 'items' arrays along axis 0\n",
    "X = clean_img\n",
    "# Create labels: 1 for 'animals', 0 for 'items'\n",
    "y = clean_label\n",
    "\n",
    "y = tfk.utils.to_categorical(y, 2) #one hot encodi\n",
    "\n",
    "# Split data into train_val and test sets\n",
    "#X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=1000, stratify=np.argmax(y,axis=1))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=.15, stratify=np.argmax(y,axis=1))\n",
    "\n",
    "# Further split train_val into train and validation sets\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=1000, stratify=np.argmax(y_train_val,axis=1))\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "#print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "# Explain\n",
    "# def getSamplesFromDataGen(resultData):\n",
    "#     x = resultData.next() #fetch the first batch\n",
    "#     a = x[0] # train data\n",
    "#     b = x[1] # train label\n",
    "#     for i in range(0,5):\n",
    "#         plt.imshow(a[i])\n",
    "#         plt.title(b[i])\n",
    "#         plt.show()\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # train generator with augmentation\n",
    "train_image_gen  = ImageDataGenerator(rotation_range=20, #maybe better not to rotate??\n",
    "                                      width_shift_range=0.1,\n",
    "                                      height_shift_range=0.1,\n",
    "                                      zoom_range=[0.5, 0.9],\n",
    "                                      shear_range=0.2,\n",
    "                                      brightness_range = [0.5, 1.5],\n",
    "                                      # vertical_flip=True,\n",
    "                                      # horizontal_flip=True,\n",
    "                                      fill_mode='nearest',\n",
    "#                                       validation_split = 0.15\n",
    "                                      preprocessing_function=add_noise\n",
    "                                      )\n",
    "\n",
    "\n",
    "# validation generator without augmentation\n",
    "# validation_image_gen = ImageDataGenerator(validation_split = 0.15,\n",
    "#                                           )\n",
    "\n",
    "train_dataset = train_image_gen.flow(x = X_train, y = y_train, seed = 42,\n",
    "                                     batch_size = BATCH_SIZE)\n",
    "validation_dataset = ImageDataGenerator().flow(X_val, y_val)\n",
    "# validation_dataset = train_image_gen.flow(x = X, y = y, seed = 42, batch_size = BATCH_SIZE, subset = \"validation\")\n",
    "# validation_dataset = validation_image_gen.flow(x = X, y = y, seed = 42,  subset = \"validation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "class LR_ASK(tf.keras.callbacks.Callback):\n",
    "    def __init__ (self, model, epochs,  ask_epoch, dwell=True, factor=.75): # initialization of the callback\n",
    "        super(LR_ASK, self).__init__()\n",
    "        self.model=model               \n",
    "        self.ask_epoch=ask_epoch\n",
    "        self.epochs=epochs\n",
    "        self.ask=True # if True query the user on a specified epoch\n",
    "        self.lowest_vloss=np.inf\n",
    "        self.lowest_aloss=np.inf\n",
    "        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "        self.best_epoch=1\n",
    "        self.plist=[]\n",
    "        self.alist=[]\n",
    "        self.dwell= dwell\n",
    "        self.factor=factor\n",
    "        \n",
    "    def get_list(self): # define a function to return the list of % validation change\n",
    "        return self.plist, self.alist\n",
    "    def on_train_begin(self, logs=None): # this runs on the beginning of training\n",
    "        if self.ask_epoch == 0: \n",
    "            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n",
    "            self.ask_epoch=1\n",
    "        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n",
    "            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n",
    "            self.ask=False # do not query the user\n",
    "        if self.epochs == 1:\n",
    "            self.ask=False # running only for 1 epoch so do not query user\n",
    "        else:\n",
    "            msg =f'Training will proceed until epoch {ask_epoch} then you will be asked to' \n",
    "            print(msg )\n",
    "            msg='enter H to halt training or enter an integer for how many more epochs to run then be asked again'\n",
    "            print(msg)\n",
    "            if self.dwell:\n",
    "                msg='learning rate will be automatically adjusted during training'\n",
    "                print(msg, (0,255,0))\n",
    "        self.start_time= time.time() # set the time at which training started\n",
    "       \n",
    "    def on_train_end(self, logs=None):   # runs at the end of training  \n",
    "        msg=f'loading model with weights from epoch {self.best_epoch}'\n",
    "        print(msg)\n",
    "        model.set_weights(self.best_weights) # set the weights of the model to the best weights\n",
    "        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print (msg) # print out training duration time\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        vloss=logs.get('val_loss')  # get the validation loss for this epoch\n",
    "        aloss=logs.get('loss')\n",
    "        if epoch >0:\n",
    "            deltav = self.lowest_vloss- vloss \n",
    "            pimprov=(deltav/self.lowest_vloss) * 100 \n",
    "            self.plist.append(pimprov)\n",
    "            deltaa=self.lowest_aloss-aloss\n",
    "            aimprov=(deltaa/self.lowest_aloss) * 100\n",
    "            self.alist.append(aimprov)\n",
    "        else:\n",
    "            pimprov=0.0 \n",
    "            aimprov=0.0\n",
    "        if vloss< self.lowest_vloss:\n",
    "            self.lowest_vloss=vloss\n",
    "            self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "            self.best_epoch=epoch + 1            \n",
    "            msg=f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % below lowest loss, saving weights from epoch {str(epoch + 1):3s} as best weights'\n",
    "            print(msg) # green foreground\n",
    "        else: # validation loss increased\n",
    "            pimprov=abs(pimprov)\n",
    "            msg=f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % above lowest loss of {self.lowest_vloss:7.4f} keeping weights from epoch {str(self.best_epoch)} as best weights'\n",
    "            print(msg) # yellow foreground\n",
    "            if self.dwell: # if dwell is True when the validation loss increases the learning rate is automatically reduced and model weights are set to best weights\n",
    "                lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "                new_lr=lr * self.factor\n",
    "                msg=f'learning rate was automatically adjusted from {lr:8.6f} to {new_lr:8.6f}, model weights set to best weights'\n",
    "                print(msg) # cyan foreground\n",
    "                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
    "                model.set_weights(self.best_weights) # set the weights of the model to the best weights      \n",
    "                \n",
    "        if aloss< self.lowest_aloss:\n",
    "            self.lowest_aloss=aloss        \n",
    "        if self.ask: # are the conditions right to query the user?\n",
    "            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n",
    "                msg='\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again'\n",
    "                print(msg) # cyan foreground\n",
    "                ans=input()\n",
    "                \n",
    "                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n",
    "                    msg=f'you entered {ans},  Training halted on epoch {epoch+1} due to user input\\n'\n",
    "                    print(msg)\n",
    "                    self.model.stop_training = True # halt training\n",
    "                else: # user wants to continue training\n",
    "                    self.ask_epoch += int(ans)\n",
    "                    if self.ask_epoch > self.epochs:\n",
    "                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n",
    "                    else:\n",
    "                        msg=f'you entered {ans} Training will continue to epoch {self.ask_epoch}'\n",
    "                        print(msg) # cyan foreground\n",
    "                        if self.dwell==False:\n",
    "                            lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "                            msg=f'current LR is  {lr:8.6f}  hit enter to keep  this LR or enter a new LR'\n",
    "                            print(msg) # cyan foreground\n",
    "                            ans=input(' ')\n",
    "                            if ans =='':\n",
    "                                msg=f'keeping current LR of {lr:7.5f}'\n",
    "                                print(msg) # cyan foreground\n",
    "                            else:\n",
    "                                new_lr=float(ans)\n",
    "                                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
    "                                msg=f' changing LR to {ans}'\n",
    "                                print(msg) # cyan foreground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to fit and use new callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*IMPORTANTE* $\\\\$\n",
    "Il modelo si deve chiamare per forza \"model\" sennò LR_ASK tira un errore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esempio su efficient net\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def make_model(img_size = [96, 96], lr = 0.01, mod_num=3):\n",
    "  img_shape=(img_size[0], img_size[1], 3)\n",
    "  if mod_num == 0:\n",
    "      base_model=tf.keras.applications.efficientnet.EfficientNetB0(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
    "      msg='Created EfficientNet B0 model'\n",
    "  elif mod_num == 3:\n",
    "      base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
    "      msg='Created EfficientNet B3 model'\n",
    "  elif mod_num == 5:\n",
    "      base_model=tf.keras.applications.efficientnet.EfficientNetB5(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
    "      msg='Created EfficientNet B5 model'\n",
    "\n",
    "  else:\n",
    "      base_model=tf.keras.applications.efficientnet.EfficientNetB7(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
    "      msg='Created EfficientNet B7 model'\n",
    "\n",
    "\n",
    "\n",
    "  base_model.trainable=True\n",
    "  x=base_model.output\n",
    "  x=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "  x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                  bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "  x=Dropout(rate=.4, seed=123)(x)\n",
    "  output=Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "  model=Model(inputs=base_model.input, outputs=output)\n",
    "  model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  msg=msg + f' with initial learning rate set to {lr}'\n",
    "  print(msg)\n",
    "  return model\n",
    "\n",
    "lr=.001\n",
    "model=make_model(lr = lr, mod_num = 2) # using B3 model by default\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=200 #quante epoche prima che si stoppi del tutto  (lasciare alto)\n",
    "ask_epoch=20 #ogni quanto viene chiesto di fermarsi\n",
    "ask=LR_ASK(model, epochs,  ask_epoch, factor = 0.6) #factor misura quanto viene ridotto il learning rate se il validation loss è più alto del più bassso trovato fin'ora\n",
    "callbacks=[ask]\n",
    "\n",
    "\n",
    "history=model.fit(x=train_dataset,  #lascia così\n",
    "                  epochs=epochs, \n",
    "                  verbose=1, \n",
    "                  callbacks=callbacks,  \n",
    "                  validation_data=validation_dataset,\n",
    "                  validation_steps=None,  \n",
    "                  shuffle=False,  \n",
    "                  initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"daspoifjdaspoidfjsaopjif\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a pipeline of:   transfer learning -> fine tuning -> full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Convnextbase = tf.keras.applications.ConvNeXtBase(\n",
    "    model_name=\"convnext_base\",\n",
    "    include_top=False,\n",
    "    include_preprocessing=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(96, 96, 3),\n",
    "    pooling='max',\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\",\n",
    ")\n",
    "Convnextbase.trainable = False\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "# Build the neural network layer by layer\n",
    "inputs = tfkl.Input(shape=(96, 96, 3))\n",
    "# x = img_augmentation(inputs)\n",
    "x = Convnextbase(inputs)\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "x=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "\n",
    "x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                    bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "# x = layers.Dense(256, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.4, seed = 42)(x)\n",
    "\n",
    "outputs = tfkl.Dense(units=2, activation='softmax',name='Output2')(x)  #you cannnot use softmax with only one neuron since it normalizes over the output neurons\n",
    "model = tfk.Model(inputs, outputs)\n",
    "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(0.05), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs=100\n",
    "ask_epoch=10\n",
    "ask=LR_ASK(model, epochs,  ask_epoch, factor = 0.5)\n",
    "callbacks=[ask]\n",
    "history = model.fit(\n",
    "    x = train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = 1000,\n",
    "    validation_data = validation_dataset,\n",
    "    validation_steps = None,\n",
    "    shuffle = False,\n",
    "    callbacks = callbacks,\n",
    "    verbose = 1\n",
    ").histbory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/content/drive/MyDrive/Homework’s/models/Convnextbase_initial\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convnextbase fine tuning 1: -32 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfk.models.load_model('/content/drive/MyDrive/Homework’s/models/Convnextbase_initial')\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#last 32 layers\n",
    "unfreeze = 32\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.get_layer('convnext_base').trainable = True\n",
    "for i, layer in enumerate(model.get_layer('convnext_base').layers[:-unfreeze]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# make sure BatchNorm layers are frozen\n",
    "for i, layer in enumerate(model.get_layer('convnext_base').layers):\n",
    "    if isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-3), metrics='accuracy')\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "ask_epoch=10\n",
    "ask=LR_ASK(model, epochs,  ask_epoch, factor = .5)\n",
    "callbacks=[ask]\n",
    "history=model.fit(x=train_dataset,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=validation_dataset,\n",
    "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/content/drive/MyDrive/Homework’s/models/Convnextbase_32\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convnextbase fine tuning 2: -64 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just change the value of unfreeze and the rest is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfk.models.load_model('/content/drive/MyDrive/Homework’s/models/Convnextbase_32')\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#last 32 layers\n",
    "unfreeze = 32\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.get_layer('convnext_base').trainable = True\n",
    "for i, layer in enumerate(model.get_layer('convnext_base').layers[:-unfreeze]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# make sure BatchNorm layers are frozen\n",
    "for i, layer in enumerate(model.get_layer('convnext_base').layers):\n",
    "    if isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-4), metrics='accuracy')\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "ask_epoch=10\n",
    "ask=LR_ASK(model, epochs,  ask_epoch, factor = .5)\n",
    "callbacks=[ask]\n",
    "history=model.fit(x=train_dataset,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=validation_dataset,\n",
    "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go on to -64, -96 etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convnextbase fine tuning final: training all model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfk.models.load_model('/content/drive/MyDrive/Homework’s/models/Convnextbase_96')\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model.get_layer('convnext_base').trainable = True\n",
    "\n",
    "model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-5), metrics='accuracy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "ask_epoch=10\n",
    "ask=LR_ASK(model, epochs,  ask_epoch, factor = .5)\n",
    "callbacks=[ask]\n",
    "history=model.fit(x=train_dataset,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=validation_dataset,\n",
    "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
