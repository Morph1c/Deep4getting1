{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMAGE PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from the 'items/' folder\n",
    "img = np.load('data.npy')\n",
    "label = np.load('labels.npy', allow_pickle=True)\n",
    "  # Normalize image pixel values to a float range [0, 1]\n",
    "img = (img / 255).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 0,1 label\n",
    "for i in range(len(label)):\n",
    "    if(label[i] == 'healthy'):\n",
    "        label[i] = 1\n",
    "    else:\n",
    "        label[i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning images from trol and shrek\n",
    "ref_img = img[58]\n",
    "ref_img2 = img[2150]\n",
    "c = 0\n",
    "c2 = 0\n",
    "rm_indexes = []\n",
    "rm2_indexes = []\n",
    "for i in range(0, len(img)):\n",
    "    deviation = np.mean(np.abs(ref_img - img[i]))\n",
    "    deviation2 = np.mean(np.abs(ref_img2 - img[i]))\n",
    "    if(deviation == 0.0):\n",
    "        #print(i)\n",
    "        c += 1\n",
    "        rm_indexes.append(i)\n",
    "    elif(deviation2 == 0.0):\n",
    "        c2 += 1\n",
    "        rm2_indexes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_img = np.delete(img, rm_indexes + rm2_indexes, axis=0)\n",
    "clean_label = np.delete(label, rm_indexes + rm2_indexes, axis=0)\n",
    "print(clean_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate 'animals' and 'items' arrays along axis 0\n",
    "X = clean_img\n",
    "# Create labels: 1 for 'animals', 0 for 'items'\n",
    "y = clean_label\n",
    "\n",
    "#y = np.concatenate([, np.zeros(len(items))], axis=0)\n",
    "y = tfk.utils.to_categorical(y,len(np.unique(y)))\n",
    "\n",
    "# Split data into train_val and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=.25, stratify=np.argmax(y, axis = 1))\n",
    "# METTERE TEST_SIZE = 15\n",
    "\n",
    "# Further split train_val into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=len(X_test), stratify=np.argmax(y_train_val, axis = 1))\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape, output shape, batch size, and number of epochs\n",
    "input_shape = X_train.shape[1:]\n",
    "output_shape = y_train.shape#[1:]\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# Print input shape, batch size, and number of epochs\n",
    "print(f\"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNETV50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = tf.keras.applications.ResNet50V2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(96, 96, 3),\n",
    "    pooling='avg',\n",
    "    classes=2,\n",
    "    classifier_activation=\"softmax\",\n",
    ")\n",
    "resnet.trainable = False\n",
    "efficent.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_augmentation = Sequential(\n",
    "    [\n",
    "        layers.RandomRotation(factor=0.15),\n",
    "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        layers.RandomFlip(),\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET TRANSFER LEARNING\n",
    "img_augmentation = Sequential(\n",
    "    [\n",
    "        layers.RandomRotation(factor=0.15),\n",
    "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        layers.RandomFlip(),\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")\n",
    "\n",
    "efficent.trainable = False\n",
    "# Build the neural network layer by layer\n",
    "inputs = tfkl.Input(shape=(96, 96, 3))\n",
    "x = img_augmentation(inputs)\n",
    "\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = resnet(inputs)\n",
    "\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "x  = tfkl.Dense(units=128, kernel_initializer=tf.initializers.HeUniform(seed=seed), activation='relu',name='Output1')(x)\n",
    "x =  layers.Dropout(0.2)(x)\n",
    "\n",
    "output_layer = tfkl.Dense(units=2, activation='softmax',name='Output2')(x)\n",
    "outputs = tfkl.Dense(units=2, activation='softmax',name='Output2')(x)\n",
    "model = tfk.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=100, restore_best_weights=True, mode='auto'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=\"/home/andrea/Scrivania/ANN/ch1/checkpoint/\",\n",
    "                                                  save_weights_only=True,\n",
    "                                                  verbose=1)\n",
    "]\n",
    "model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "tl_history = model.fit(\n",
    "    x = X_train, # We need to apply the preprocessing thought for the MobileNetV2 network\n",
    "    y = y_train,\n",
    "    batch_size = 32,\n",
    "    epochs = 100,\n",
    "    validation_data = (X_val, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
    "    callbacks = callbacks\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ResenetV50')\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
